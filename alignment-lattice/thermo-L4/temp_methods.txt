
    def _parse_S(self, s: str) -> dict:
        """Parse structured S into dictionary"""
        parts = [p.strip() for p in (s or "").split("|")]
        d = {}
        for p in parts:
            if ":" in p:
                k, v = p.split(":", 1)
                d[k.strip()] = v.strip()
        return d

    def calculate_slot_fidelity(self, S_dict, reconstruction, full_context) -> float:
        """
        Slot-wise fidelity scoring:
          - 0.40: L0-L3 phrases verbatim (0.10 each)
          - 0.15: Novelty type named
          - 0.15: All deltas mentioned
          - 0.10: Testable prediction
          - 0.20: Base similarity
        """
        rec = (reconstruction or "").lower()
        score = 0.0
        
        # L0-L3 verbatim (40%)
        for level in ["l0", "l1", "l2", "l3"]:
            if level in S_dict:
                phrase = S_dict[level].replace("_", " ").lower()
                if phrase and phrase in rec:
                    score += 0.10
        
        # Novelty mapping (15%)
        novelty_map = {
            "ugf": "unified gradient flow",
            "sim": "simultaneous processing",
            "cli": "cross-level invariant",
            "raf": "reduced attention fragmentation"
        }
        l4n = S_dict.get("l4n", "")
        if l4n in novelty_map and novelty_map[l4n] in rec:
            score += 0.15
        
        # Deltas (15%)
        if all(x in rec for x in ["latency", "entropy", "token"]) and ("energy" in rec or "free energy" in rec):
            score += 0.15
        
        # Testable prediction (10%)
        if "test:" in rec or any(w in rec for w in ["predict", "measure", "should", "if "]):
            score += 0.10
        
        # Base similarity (20%)
        base = self.calculate_combined_fidelity(full_context, reconstruction)
        score += 0.20 * base
        
        return min(score, 1.0)

